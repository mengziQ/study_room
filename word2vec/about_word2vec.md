# word2vecについて  
パパンにword2vecについて質問されて答えられなかったので頑張って理解したい、あとできれば実装もしたい（希望）  
この文書はword2vecの概要・仕組みについて参考文献を見ながらまとめただけのものです。  

## 参考文献  
[Word2Vec のニューラルネットワーク学習過程を理解する](http://tkengo.github.io/blog/2016/05/09/understand-how-to-learn-word2vec/)  

## word2vecとは  
- 隠れ層と出力層の２層からなるニューラルネットワーク  

## 学習について  
- 学習方法：NNにたくさんの単語を読み込ませる(？)  
- 学習後にNNが獲得するもの：入力データ（複数の単語）に対する良い表現をベクトルの形で獲得する(?)  
- 学習高速化のテクニック： 次の２種類： Hierarchcal Softmax, Negative Sampling  

## NNのアーキテクチャ  
2種類ある。(つまり、効率の良い学習が行えるネットワーク構造として、以下２種類が論文で提示されている。)  
1. CBoW(恐らく、Convolutional Bag of Words)  
2. Skip-gram  

## Skip-gramでword2vecをモデル化する方法      
### Skip-gramとは  
「ボキャブラリー」と呼ばれる単語の集合の各単語について、任意の単語の周辺に出現する確率を予測するモデル。  
※ word2vec自体はNNで、これも広義のモデルだけど、NNの中で学習（？）予測（？）の際に使用されている根幹のモデルの１つがこのSkip-gramという解釈。  

