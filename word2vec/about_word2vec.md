# word2vecについて  
パパンにword2vecについて質問されて答えられなかったので頑張って理解したい、あとできれば実装もしたい（希望）  
この文書はword2vecの概要・仕組みについて参考文献を見ながら自分の頭の整理のためにまとめただけのものです。文字だけで分かりづらいです。  

## 参考文献  
[Word2Vec のニューラルネットワーク学習過程を理解する](http://tkengo.github.io/blog/2016/05/09/understand-how-to-learn-word2vec/)  

## word2vecとは  
- 隠れ層と出力層の２層からなるニューラルネットワーク  

## 学習について  
- 学習方法：NNにたくさんの単語を読み込ませる(？)  
- 学習後にNNが獲得するもの：入力データ（複数の単語）に対する良い表現をベクトルの形で獲得する(?)  
- 学習高速化のテクニック： 次の２種類： Hierarchcal Softmax, Negative Sampling  

## NNのアーキテクチャ  
2種類ある。(つまり、効率の良い学習が行えるネットワーク構造として、以下２種類が論文で提示されている。)  
1. CBoW(恐らく、Convolutional Bag of Words)  
2. Skip-gram  

## Skip-gramでword2vecをモデル化する方法      
### Skip-gramとは  
「ボキャブラリー」と呼ばれる単語の集合の中の、任意の単語の周辺に出現するたくさんの単語の各出現確率を予測するモデル。  
最後の「たくさんの単語」は参考文献中のNに該当。つまり文字ベクトルの次元数。  
※ word2vec自体はNNで、これも広義のモデルだけど、NNの中で学習（？）予測（？）の際に使用されている根幹のモデルの１つがこのSkip-gramという解釈。  

数式をmdで書くのが恐ろしく面倒くさいので、詳細は参考文献を見てください。  

#### Skip-gramの内容を数学に置き換えると その1:条件付き確率    
まず、条件付き確率に置き換える。  
何故かというと、例えば{'おそ松','さん','かわいい'}のボキャブラリーの場合、  
- 「おそ松」のあとに「さん」「かわいい」が来る確率：　直感的には高め  
- 「さん」のあとに「かわいい」が来る確率：　直感的には高め  
- 「さん」のあとに「おそ松」が来る確率：　直感的には低め  

こんな感じで、Skip-gramがやりたいことは条件付き確率問題と言えそう。  

そしてそれを更にSoftmaxに置き換える。  
何故かというと、出力層にて出力形式を確率にしたいから。だと思います。。(NNの活性化関数)      

条件付き確率のベイズの公式がなんでSoftmaxに置き換えられるかを知るには[このサイト](http://owatank.hatenablog.com/entry/2017/11/03/151956)が良いと思いました。  

ざっくり言うと、ベイズの公式の分母は、条件付き確率の「条件」の方の事象の確率（周辺確率と言うっぽい）なのですが、これは「条件」の事象が起こった際のあらゆる事象の確率の総和であるため、softmaxの分母と似た式になるのです・・・（伝わらない）  

これは私の頭のメモに過ぎないので、詳しくはさっきのリンクを見ましょう。  

#### Skip-gramの内容を数学に置き換えると その2:同時確率
これを発展させる。C(コンテキスト)というものを導入する。  
Cを導入して、任意の単語の前後◯単語を予測するモデルにする。(Cは１や２など様々な数字を設定でき、数字は左の文章の◯に該当する。)  

恐らく「前後」にした関係で、条件付き確率ではなく同時確率に置き換えたと思ってます。。。でも式もそのままベイズの定理（からのSoftmax）を適用できるのがちょっと理解しきれないです    

コンテキストを導入して同時確率に置き換えたら、その確率を最大化するようなベクトルをNNを使って獲得していく！というのがword2vecだと思ってます。  


## word2vecにおけるニューラルネットワークの構造  
上にも出てきた通り、word2vecは隠れ層と出力層の２層のNNです。(こういう時入力層はカウントしないの？)  
入力層→隠れ層、隠れ層→出力層は全結合です。  

今まで書いてきたSkip-gramモデルをこのネットワーク上で動かして、最適な単語ベクトルを学習していく、といったイメージです。  

この辺りからN=単語ベクトル次元数、と出てくるようになるのですが、多分word2vecが持つ語彙数みたいなものだと予想してます。(よく分からん)  

### 入力層  
入力層もベクトルです。  
この段階ではベクトルの次元は「ボキャブラリー数」。  
任意のボキャブラリーの入力ベクトルは、その単語の次元のみ１となっていて、その他が全部0のベクトルです。  

#### 実装のイメージ  
つまり、何かしらの文章をword2vecで分析(もしくは学習)したい場合、文章内の全単語をこのベクトルに変換するプログラムかなんかが必要。    

### 隠れ層  
ここは、入力ベクトルに対して重みをつける層みたいなイメージです。  
各ボキャブラリー専用の単語ベクトル(参考文献では「重み」とも呼ばれている)が並んでいる層です。  
例によって、個人的に謎なN次元の文字ベクトルがボキャブラリー数分横に並びます。  
というのも、



